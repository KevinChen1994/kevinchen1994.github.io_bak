---
title: "GPT系列语言模型"
date: "2023-08-16T11:09:00.000Z"
lastmod: "2025-02-20T10:02:00.000Z"
draft: false
featuredImage: "https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-8\
  2ce-4f96-ae1a-879bd6c9f3a6/ebd5585c-a296-472f-ab99-07a182282b30/v2-0816de4e29\
  410803bf955353146bceea_1440w.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Conte\
  nt-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466WW6PLRRO%2F20250222%2\
  Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152218Z&X-Amz-Expires=3600\
  &X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXV\
  zLXdlc3QtMiJIMEYCIQCSkN0%2BA4erkbXpW5G2op2AkPr%2F2f09l8jo9owW%2Bec6gAIhAODj%2\
  BCbHdIlOBDAaA6wQV5V7ORwA6QbHNRH4FGsNuRQwKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F\
  wEQABoMNjM3NDIzMTgzODA1IgwsSGokylqlCkOdHFEq3AOVJM8T2XKJdDIs0ZlAnqRcX5YCRdfpOg\
  MNwC1bShMGqVjLLSxxGRzEXJUJbarYgWGhMU5jYVJ5diwtfU3i2%2BocEMEIQVf88fj005I6hC9bt\
  qwANx1PlEBrtMq8pZ1SLSA2WcU6UY5mn6eXmADP%2FSc8jRLCQxoNHDWvvuJCLFGbXZiUU4DXth2o\
  KmPncederOF%2F48abMYp6GBOLFbtT7gCD2V4aXM0lUvYhYTuqFXFHPkNwenOU2MiHXRPOt2RF6gn\
  Sux5v57g0amEwmSIjIy5HdgsLEzGY6vdP%2Bp3Ahv5vcfIbwvLP%2BK5%2Bqez8UXrcGk49pF5WnV\
  4RweyTppRtQYSCyjN%2BZKc%2FgUTDVpS1JrsptvwFpdQ%2Bx8NfyXySP93tZDi2nInMzc%2FmHG7\
  gMXlaFRU5iBM87pIUG7cAcE6jNXLBt9kH5kD8QQiVkGpKBu%2BR9YxhSy7n1JDIqTVVJLLNs7z13E\
  hNqzyrXO8oDz1JEbb2IboJaU0kTcyTT2Ybf6Nuh6GKDNpGq4uOS%2BphLVvzmBbhxl690NyzuHKHp\
  NIjcz5fshzerjo8Bq4w3%2FulxvpKWx6bV0toXCPoJeUqHJ8epTbb9KmWEXDw%2Ft3kaK7%2B4%2F\
  SRUcT56RAujZQhYIY3IDCZ5ua9BjqkASoNoptYlOPSA%2F3FqSt6QvG5GaP6skEG7QinmfPH%2FxM\
  q%2FYSr2qjb7x3HRekYu9GDyxaFDS%2BUzHI2KUcuDjhzViJrJUYBwtZXC4A4bIhfIkCeZN2kZiur\
  hym1w3G8LjNfcHUr8tRsoDKfq0gMK6rYiRCBTyi0u3y894IyOFTvH2B7DPtHsbFrCTNfDtGQu%2BM\
  OiTgBPjx2lzln9SQpkJg7Ew9JaE6l&X-Amz-Signature=00f1c131476f0a90dd67480cf2de1c4\
  cb9fd5041f2c9c166bdebe32cba1835e3&X-Amz-SignedHeaders=host&x-id=GetObject"
series:
  - "Tech"
authors:
  - "陈猛"
tags:
  - "GPT"
categories:
  - "LLM"
summary: "科普，无技术技术分享，像讲故事一样根据时间线来介绍GPT1到GPT4。"
NOTION_METADATA:
  object: "page"
  id: "2c3b3478-25b5-4dd8-8b09-e3172090d43a"
  created_time: "2023-08-16T11:09:00.000Z"
  last_edited_time: "2025-02-20T10:02:00.000Z"
  created_by:
    object: "user"
    id: "cc08a802-cdc1-4040-b261-957206a41bd5"
  last_edited_by:
    object: "user"
    id: "cc08a802-cdc1-4040-b261-957206a41bd5"
  cover:
    type: "file"
    file:
      url: "https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-a\
        e1a-879bd6c9f3a6/ebd5585c-a296-472f-ab99-07a182282b30/v2-0816de4e294108\
        03bf955353146bceea_1440w.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Con\
        tent-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466WQLA4HBR%2F20\
        250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152034Z&X-Am\
        z-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMP%2F%2F%2F%2F%2F%\
        2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQCCElXdqTWEBnc8sQRUfaaxKYLkQfOJrq\
        oPbZ%2BC9wOiwwIhAMgHiO%2F9%2Bcldw2EjuLkII1plJpUDYxZnhaOCNTQTpP0xKogECOz\
        %2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyOyQkm%2FdruTXha\
        lWIq3AMwTyU6k19gZ%2BrdsUkEW3cxI5grW6qhOFdzrA0bsIPyzcULFk94Zz43ZVbmeuuNy\
        JAHAH8wU5v%2BmGxKQ7ZPNaMM6ovlUhxvJaxd4lItkFLfCFIr3OgBrPMEwHZ1snsnMaxKEl\
        pIRsG1XhX6%2FXsufvcB%2FQjhaj5bzfitPnc8EjK6qsFRqic9E%2FBFZJKiW0gug8q0xGJ\
        Pplpfj%2FL9FKIJmCERbC4IH%2FY6z2uX1tYEji9DVm4FMayj8utcSfwQKDZwUXQW0kqwCK\
        HjY6Om9F5nQ6%2BrEnKlwxg0x6ax9MjfXVVvFhdz4SBsQJL8j48UEvtiZ851bcr4d56IDbd\
        DOTbpdlfFl1xEWTkjltaPmgs6U3kAgCvmjmhFznH1Wh7o16gv1xIjRC7Ae%2Brofz6MGfCB\
        5NEMmFKHrr2JDKLbbXmZxMK3MDh%2FjY4mqysm%2ByAtxc%2BBj1mAfSXOOdo2LpHEuxmkR\
        isniHvfVvYMaUkGDyKmvbRWDB%2FRS8lKFw%2FkUoBYZ%2FXZNu4G7JEqBqlRKIPV2KoEDg\
        B%2FuH7bN2rXIPziE2mghNsLOv%2Be98JPU0CeenZ4qM7PVBnQa%2F2T9nepiEpK7w9DTpd\
        ndqps%2FqYyqGMq6taIg3hmfd7msgj4N9SqD08tvjDN3Oa9BjqkAQfb1XD5ydPs39h3LWRw\
        ldMt8RF5lDe6SXg4Kid61pReGR9sYCL%2BoadyDRPSKjMtWJsw%2BgYxRuEK5olbI3d5skz\
        keqNqfxH99mObLQ4NWV1Bbx6JLZVWQjprfZqjFDmIwxh0gdTDIFJ0%2BpE3I%2Byqh6Mkeg\
        V%2Fx82KVPkUuNgMwwGBqWiwh26uen%2BM%2FoIBYbZcXr1YKaOO8nIfxeEEvZ4FSeVdpAj\
        E&X-Amz-Signature=577e95875327f57aff1ad663fbe28971aa00e6bd308672826a2af\
        e297dc809bb&X-Amz-SignedHeaders=host&x-id=GetObject"
      expiry_time: "2025-02-22T16:20:34.553Z"
  icon: null
  parent:
    type: "database_id"
    database_id: "8d6a6f9d-5a2c-433b-a560-b744eab9db1a"
  archived: false
  in_trash: false
  properties:
    series:
      id: "B%3C%3FS"
      type: "multi_select"
      multi_select:
        - id: "f6345faf-6e79-413e-847a-3fb764a61e06"
          name: "Tech"
          color: "green"
    draft:
      id: "JiWU"
      type: "checkbox"
      checkbox: false
    Created time:
      id: "UBQ%7B"
      type: "created_time"
      created_time: "2023-08-16T11:09:00.000Z"
    authors:
      id: "bK%3B%5B"
      type: "people"
      people:
        - object: "user"
          id: "cc08a802-cdc1-4040-b261-957206a41bd5"
          name: "陈猛"
          avatar_url: "https://s3-us-west-2.amazonaws.com/public.notion-static.com/775523\
            b7-57cf-4c98-8ad8-8777d898666f/notion-avatar-1678713535269.png"
          type: "person"
          person:
            email: "346521888@qq.com"
    custom-front-matter:
      id: "c~kA"
      type: "rich_text"
      rich_text: []
    tags:
      id: "jw%7CC"
      type: "multi_select"
      multi_select:
        - id: "b7a3cd71-5e79-402d-bc67-4104b0f3eaab"
          name: "GPT"
          color: "yellow"
    categories:
      id: "nbY%3F"
      type: "multi_select"
      multi_select:
        - id: "e417d9a1-8454-498a-b9de-502d57e26681"
          name: "LLM"
          color: "gray"
    summary:
      id: "x%3AlD"
      type: "rich_text"
      rich_text:
        - type: "text"
          text:
            content: "科普，无技术技术分享，像讲故事一样根据时间线来介绍GPT1到GPT4。"
            link: null
          annotations:
            bold: false
            italic: false
            strikethrough: false
            underline: false
            code: false
            color: "default"
          plain_text: "科普，无技术技术分享，像讲故事一样根据时间线来介绍GPT1到GPT4。"
          href: null
    Date:
      id: "zYLY"
      type: "date"
      date: null
    Name:
      id: "title"
      type: "title"
      title:
        - type: "text"
          text:
            content: "GPT系列语言模型"
            link: null
          annotations:
            bold: false
            italic: false
            strikethrough: false
            underline: false
            code: false
            color: "default"
          plain_text: "GPT系列语言模型"
          href: null
  url: "https://www.notion.so/GPT-2c3b347825b54dd88b09e3172090d43a"
  public_url: "https://kevinchen1994.notion.site/GPT-2c3b347825b54dd88b09e3172090d43a"
UPDATE_TIME: "2025-02-22T15:22:22.365Z"
EXPIRY_TIME: "2025-02-22T16:22:16.803Z"

---
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">


## 前言


最近在部门做了一次GPT系列语言模型的分享，这次分享受众很大一部分没有算法背景，所以这篇文章想用讲故事的方式给大家介绍GPT系列语言模型，可以称为无技术技术分享。


想必大家最近都被ChatGPT刷屏了吧，那为什么ChatGPT这么智能呢？其实最主要的还是因为他的底座模型很强大，所谓RLHF都是为了让底座模型学习到人类的交互方式，让它更好的相应人类的指令。那接下来我会沿着GPT发布的时间顺序分别给大家介绍一下这几个模型。


![图1 语言模型发布时间线](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/a8f7f1b0-2509-45a9-82a8-da0f4d743256/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=d3aec8ade59b6594d06d30db894eb3dce16926c1bf733a5935eaada5f07b7564&X-Amz-SignedHeaders=host&x-id=GetObject)


## Transformer 梦开始的地方


首先我们先简单介绍一下Transformer，因为GPT中的T就是指Transformer，这是大模型梦开始的地方。


2017年，Google发表了一篇论文《Attention is all you need》，这篇文章主要讲他们设计了一个新的网络结构，以往我们使用的网络结构基本就是卷积神经网络（CNN）、循环神经网络（RNN），而Transformer只使用了self-attention机制，基于seq2seq结构设计了一种新的网络。


所谓seq2seq结构就是网络具有编码器和解码器，编码器就是将我们的输入进行编码，提取特征，而解码器就是对编码进行解析，输出结果，如果是做中英翻译任务的话，输入就是中文，然后编码器对中文进行编码，解码器会对编码进行解码，输出就是对应的英文。这是整个Transformer的结构图，左边是编码器，右边是解码器。


![图2 Transformer结构图](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/3c5ae160-94ed-4a2f-858d-1a4857533b9f/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=4c99207dc5f980c88e4990a2680e5106631213d3ec2cfc1429829e390fb95594&X-Amz-SignedHeaders=host&x-id=GetObject)


什么是 self-attention 呢，翻译过来就是自注意力，自己跟自己算 attention，具体的做法就是将输入转换成三个向量，分别是K、Q、V，然后K与Q进行点乘、缩放，再经过 softmax 以后得到一个权重矩阵，在与V相乘，V拿到的就是加权后的向量，也就是他该关注哪里，不该关注哪里都计算好了。这样的好处就是可以忽略文本之间的距离问题，直接计算全局的特征。举一个简单的例子，一段文本中说：小明是一个好学生，他上课认真听讲，下课好好做作业，然后巴拉巴拉说了一堆，最后说他真值得我们学习。这句话在计算self-attention 的时候，小明这个名字就与最后的他这个字关联性很高，即使他们之间的距离很远，Transformer也关注到了他就是小明，如果使用的是RNN，这么远的距离可能会遗忘掉小明这个名字。


所谓多头自注意力，就是我们有多对KQV，他们会分别进行attention，之间互不干扰，这样我们就能拿到多种特征，从而提高了模型的特征抽取能力，并且多头自注意力之间还是并行计算的，也就是能够更好的利用 GPU 资源，提高我们的计算效率。


![图3 自注意力机制和多头自注意力机制](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/93f9310e-784a-4ed2-b810-da5c2f86f6e4/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=fba58842a0b81cb1c8dc6f69c553af0c7d65e41c0404a91d8461e4e6e55cf8e9&X-Amz-SignedHeaders=host&x-id=GetObject)


具体细节大家不需要过多关注，只需要了解Transformer相比于之前的CNN和RNN，不仅特征抽取能力强、还并行度高，还能更好的关注全局信息，减少学到后边忘了前边的这种错误，解决了长距离依赖的问题。


基于这些能力，使我们训练像GPT和BERT这种大模型成为了可能。


## BERT与GPT1 既生瑜何生亮


在2018年openAI发布了GPT1，采用的方法是使用Transformer-decoder的架构，也就是只使用了Transformer的解码器，可以通过左边的图看出来，GPT1的训练过程是单向的，因为GPT1的训练方法是单字接龙，也就是根据上文预测下文，从这个例子大家可以看到，通过输入的招字，GPT1预测聘字，然后通过招聘预测饿字，以此类推直到预测结束。通过这种训练方式，GPT1能够学习到文本之间的语义知识。一般我们称这种模型为自回归模型，他更适用于生成类的任务。


紧接着Google发布了BERT，大家对BERT可能或多或少都听说过，我们很多工作都是基于BERT去做的，BERT是基于Transformer-encoder架构设计的，也就是只使用了Transformer的编码器，从图中可以看出来，BERT的训练过程是双向的。因为encoder可以看到输入文本上下文的全部信息，所以需要在训练时mask掉一些字，因此BERT的训练方法为完形填空，让模型根据上下文进行预测mask掉的字。一般我们称这种模型为自编码模型，他在自然语言理解任务上会有不错的效果。


![图4 BERT与GPT1模型结构对比](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/8c392985-ebf8-4359-b9eb-46054973a14d/WechatIMG96113.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=7e0a3bfcf118f402ebc4db1d87450074cf10b27f1ff852d819165b13c98d9fc4&X-Amz-SignedHeaders=host&x-id=GetObject)


![图5 BERT与GPT1训练方式对比](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/17efa5e9-49b8-47de-896c-322f87b689c7/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=c2b55e9cd5bc9c1ba7d357e8e6faa9f01f4a4a5b4a7335763510f14f7f6a8686&X-Amz-SignedHeaders=host&x-id=GetObject)


通过对比我们可以看到两个模型设计是很像的，只不过互为对立面，一个使用的是encoder，一个使用的是decoder。我们再来看一下两个模型的参数，由于GPT1是先发布的，所以BERT完全按照GPT1的参数量进行了设计，都是12层transformer，768维的隐藏层和12个自注意力头。那这两个模型的参数量几乎一致的，他们俩当时都刷新了各大NLP任务的榜单，所以我给这一节的标题起名为既生瑜何生亮。


![图6 BERT与GPT1参数对比](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/539fecc7-8111-4846-aa76-3b4409669d7d/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=719310b00cd2398d207fbb20979b8fba01287a0c28678d73ec64498007d7c08a&X-Amz-SignedHeaders=host&x-id=GetObject)


GPT1与BERT的使用方法都分为两个阶段，第一阶段都是在大规模无标注语料上进行预训练，也就是pre-training，就是我们刚才提到的单字接龙和完形填空，这阶段的目的就是通过预训练获取无标注语料中的语义知识。这一步Google和openAI都替我们训练好了，我们可以直接下载他们开源的模型，第二阶段是根据不同的下游任务，使用我们自己的有标注的数据进行有监督训练，也就是fine-tuning。这样的做的目的就是利用在预训练阶段模型学习到的语义知识，通过fine-tuning把这部分知识迁移到下游任务中。


下图为BERT的使用方法，根据不同的任务，在BERT后添加不同的网络层就可以了，在输入部分通过添加特殊标识符[CLS]表示句子的向量，如果是双句任务，通过[SEP]来分割两个句子。例如做分类任务，直接拿到BERT的输出，添加一个分类层就可以；序列标注任务中，我们可以拿到每一个字的输出，在通过分类器进行判断标签即可。


![图6 BERT使用方法](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/76396371-b8f3-4381-861f-321af06f54bb/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=f9da878ca2a7685baffdba728b3bd9d4ae7aea451fb3a221b997d0eff7f50040&X-Amz-SignedHeaders=host&x-id=GetObject)


下图为GPT1的使用方法，使用方法也很类似，通过特殊标识符标识整个句子，双句任务按照特殊标识符进行分割，输入给模型，在拿到模型的输出后，拼接一层线性层就可以拿到预测结果。


![图7 GPT1使用方法](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/7be7405a-aa13-45b8-8f29-9d98dfc60967/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=8ae451cb71994bc6fb1ee635f65ec09ba53326a1b791fd4952ecbe2c6a927cb4&X-Amz-SignedHeaders=host&x-id=GetObject)


我们来看一下实验结果。


BERT-base为与GPT1参数规模相同的模型，可以看到在多项NLP任务中，他的成绩都优于GPT1。


GPT1的效果比之前的方法都要好，但是由于BERT的训练方法为完形填空，所以学习到的语义知识更多，因此在下游任务中的表现都是优于GPT1的。所以在此以后，BERT大火了起来，成为了我们最强的baseline，无论什么任务，我们优先都会去尝试BERT的效果怎么样，我们一度以为BERT才是未来的方向，当然后面的故事大家都清楚了。


![图8 BERT与GPT1实验结果](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/1ceda2a3-63d7-4567-8036-d3951273f29f/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=80f0bbb55ee3c763dad53ef28d04b0cd999b60fe24f7454f90b77e725f95cf6d&X-Amz-SignedHeaders=host&x-id=GetObject)


## GPT2 无监督学习者


在BERT发布以后，openAI痛定思痛，决定继续深挖自回归模型的潜力，于是在2019年GPT2发布了，论文名称为Language Models are Unsupervised Multitask Learners，语言模型是无监督的多任务学习者。主打的是Zero-Shot learning，所谓Zero-Shot learning就是指在预训练完成后，不需要任何标注的数据，直接对下游任务进行预测。openAI这次想体现的是模型泛化能力强。


想要做的Zero-Shot，那前提就是模型一定要强，所以openAI加大了参数量和训练数据，参数量提升到了1.5B，也就是15亿的参数，相比于GPT1和BERT提升了13倍多，训练数据也由之前的4GB提升到了40GB。通过实验对比，在Zero-Shot的设定下，GPT2在多个任务上能够吊打绝大部分模型，当然也有一些任务表现没有那么好。


![图9 GPT2在zero-shot任务上的表现](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/9d434d51-ec7e-4afa-8805-4d722c579e5a/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=f42de365b5c8b7d34e4561b27652b9aa321ef2e950c09e7f5eedc49b246c1f8b&X-Amz-SignedHeaders=host&x-id=GetObject)


使用生成式的模型如何进行预测呢？在GPT1中因为使用了微调的方法，所以在输入文本中添加了一些特殊符号，这样模型可以记住这些符号，而在GPT2中，我们直接进行预测，并没有这些特殊符号，所以使用方式为直接写出需要模型做的任务，模型自动识别并输出结果，翻译任务：(translate to french, english text, french text)；阅读理解任务：(answer the question, document, question, answer)。这种方式也称为prompt，也就是提示模型他需要做什么，prompt的目的其实就是为了激发模型的补全能力，我们的prompt作为模型的输入，让模型输出我们想要的东西。后来这种方式成为了大模型主要的使用方法。


## GPT3 大模型的顿悟时刻


实际使用场景下，大家其实都能提供一些标注数据，人类在学习的时候有一些样本学习的也会很好，于是openAI继续深挖，发布了GPT3，论文名称为Language Models are Few-Shot Learners。所谓Few-Shot就是指训练完成后，给我少量的标注数据，但是不进行微调，只是让我看几个例子，我就能帮你做下游任务。


这次GPT3的参数量到了175B，也就是1750亿参数，相较于GPT2提升了100多倍，训练数据到了570GB。在这也样数据量和参数量下，GPT3展示了什么叫大力出奇迹。除了在很多下游任务上有不错的表现以外，还解锁了其他的能力。


### In-Context Learning


什么是In-Context Learning呢，简单来说，就是模型在不更新自身参数的情况下，通过在模型输入中带入新任务的描述与少量的样本，就能让模型”学习”到新任务的特征，并且对新任务中的样本产生不错的预测效果。这种能力可以当做是一种小样本学习能力。可以参考下图的例子来理解：在预测阶段，在输入中输入几个任务样例，这样模型就能在不调整模型参数的情况下学习到该任务需要解决什么问题，你再问类似的问题，模型就会输出正确的答案。


![图10 In-Context Learning](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/30eea9a3-e501-40ce-a953-ef07fdfe5b6f/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=7cce0130156ce6df7b7558e0f18ed3c1a293478712e46e0f52e8b17ec567efc4&X-Amz-SignedHeaders=host&x-id=GetObject)


我们可以看一下实验结果，In-Context Learning在大模型上随着标注样本的增多，最多也就几十条，准确率能够稳定提升，这里大模型指千亿参数以上，在小模型上虽然也有提升，但是效果没有大模型效果明显，所以这个能力是大模型独有的，所以这里我们称其为大模型的顿悟时刻。


![图11 不同大小模型的In-Context-Learning的能力](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/1f5c5dbb-5839-40b4-b874-c0f3bf61eeab/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=e695e1e115f1469880127f257269033c40fee6bd34b0c1ff9fbfac4134f4335d&X-Amz-SignedHeaders=host&x-id=GetObject)


### chain of thought


我们再来讲一下大模型解锁的另外一个能力，chain of thought，也就是思维链。在一般的prompt中，对于一个复杂或者需要多步计算的问题，如果你直接问模型，他大概率会直接给你一个错误答案，例如左下角这个例子。如果你给出样例，但是没有提供推导过程，模型也很难给出正确答案，例如左上角这个例子，绿色是给了一个样例，然而在继续问一个新的问题的时候，模型直接输出了结果，但是结果错误了。而右上角这个图中展示了，我给了你一个样例，还给了你推导过程，再问你一个新问题的时候，模型不仅能输出解题过程，还能计算正确。这就是思维链的能力，让模型具有了像人类处理复杂问题一样的思维能力。


甚至还有人提出了一种方法，在问问题的时候，不告诉模型推导过程，只是跟模型说一句，Let’s think step by step，模型就能给出推导过程和正确答案，这妥妥的模型鼓励师了，鼓励模型不要着急，一步一步想，模型就能给出正确的答案。


![图12 chain of thought](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/19264f3d-9727-4779-95bc-526b8cbe3fb4/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=787b6da2cfe336e4469921e9050488d931a9ba7df94a4a9e9b749d8dda6d8be1&X-Amz-SignedHeaders=host&x-id=GetObject)


## 能力涌现


In-Context Learning 和 chain of thought 的能力是怎么来的呢？目前学术界也没有确定的答案，现在大家称这种现象为大模型的Emergent Abilities，也就是能力涌现。一般模型的大小和模型的能力是有固定比例的，一般是称线性关系，即模型越大能力越强，但是当模型的规模到了一定的程度以后，像是GPT3、chatGPT这种千亿级别的大模型，模型的能力会有一个质的变化，也就是能力涌现。


可以通过实验结果图来看一下，横轴为模型的计算量，我们可以理解为模型参数量越大，模型的计算量也就越大，纵轴为模型在不同任务上的表现。随着模型规模变化，模型的能力开始是一点点提升的，有的任务甚至是一条水平线，但是当模型的规模到了一定的程度，模型的能力突然就提升了很多。这也许就是大力出奇迹吧。


![图13 Emergent Abilities](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/0d9bc725-4591-4150-813c-9638b7ec7f30/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=0c1293b97d5fc37a1b44678d4869eb946e545420f9f3892a8d614cb16ed07d78&X-Amz-SignedHeaders=host&x-id=GetObject)


## GPT3.5 守得云开见月明


GPT3已经能帮我们做很多事情了，但远没有到chatGPT那么强，我们知道chatGPT是基于GPT3.5进行训练得来的，chatGPT之所以这么强，是因为他有一个强有力的底座模型，也就是GPT的守得云开见月明的时刻。GPT3.5并不单单指一个模型，而是一系列的模型，通过openAI官网我们可以看到，这一系列模型分别擅长不同的任务，有的善于理解代码，有的善于执行人类命令，有的善于聊天。


![图14 GPT 3.5系列模型](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/47d4bc8e-1e87-4b62-bc20-9c37fa343b65/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=faead5981e20c894efce424ae19004efd79d3ebbf70c81275474b3f2cc55c961&X-Amz-SignedHeaders=host&x-id=GetObject)


目前openAI没有公开GPT3.5具体是多大规模和怎么训练的，不过通过模型擅长的任务，GPT3.5可能不仅仅增大了模型参数量和训练数据量，还加入了代码的训练数据，并且加入了instruction tuning。


加入代码训练数据我们很好理解，让模型能够读懂代码，能够帮助我们生成代码和修改代码bug，还有一些研究表明在加入了代码训练以后，模型的逻辑推理能力能够有效的增强，原因可能是因为代码都是逻辑比较清晰的，模型通过学习代码，从而增强了自己的逻辑推理能力。


那什么是instruction tuning呢？我们先来回顾一下BERT、GPT1的使用方法，也就是fine-tuning，针对不同的下游任务，使用不同的标注数据对模型进行微调，此时模型的参数是进行调整的。


在GPT2和GPT3中，因为模型参数量太大了，一般人也没有那么多的计算资源去优化，也很有可能越训练模型效果越差，所以模型的参数是不允许被修改的，大家是通过prompt也就是提示去命令模型帮助我们解决问题，prompt的目的其实就是为了激发模型的补全能力，我们的prompt作为模型的输入，让模型输出我们想要的东西。


instruction tuning直接翻译就是指令学习，是指使用将有标注的数据集使用自然语言描述的方式对模型参数进行微调，可以使用多个不同任务的数据集对模型进行指令学习，这样做的目的不是为了让模型学习到标注数据中的知识，因为在预训练阶段模型已经学习到了充足的知识，这样做是为了让模型能够更好的响应人类的指令，从而能做出正确的反馈。


![图15 instruction tuning](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/cd3275a5-4d7e-4d53-948e-36b518e13eff/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=b628f0cdaeb14f45700393e2c1b82e74becf934a819a3b55fe50685aced602ce&X-Amz-SignedHeaders=host&x-id=GetObject)


当然GPT3.5还有使用到instructGPT的训练方法和强化学习的训练方法，这里我们不做介绍。


## GPT4 更高更快更强


就在3月15号，openAI发布了GPT4，这里我们需要弄清楚GPT4与之前的模型都不一样，因为GPT4不单单指一个模型了，他对标的是chatGPT，是一套系统，相较于chatGPT他更加强大和安全了。


![图16 GPT4](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/e1d1b71e-0df3-43f5-b5f9-3a7f4098dbbd/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=f15f523a0a3fd9f1b03052ab524b35502992e5a2c381371ed98fe9800026604f&X-Amz-SignedHeaders=host&x-id=GetObject)


由于openAI没有公布GPT4具体的参数和训练方法，所以这里我们简单介绍一下GPT4强大在哪里。


首先是多模态，GPT4支持文本和图片的输入，输出只支持文本，你可以通过输入图片让他解释这张图里都有什么，也可以给他一张搞笑图片让他给你解释为什么好笑，这里体现了GPT4强大的常识知识，也就是他会有类似人类的认知，知道一张搞笑图片搞笑在哪里。


再就是更强大的编程能力，通过发布会的视频，我们看到了一个例子，演示者通过手绘了一张网页的草图输入给GPT4，GPT4很快就生成了这个网页的源代码，拿过来可以直接进行部署。他还支持你描述一个游戏的玩法，他去给你生成游戏的源代码。


GPT4支持更长的输入，chatGPT最长是4000个token，当你们之间的对话长度超过这个值，他就会丢失一些上文的信息，从而输出的内容可能会不符合你的要求，GPT4直接升级到了3.2万个token，大约25000个字，所以能更好的支持对轮对话的内容。


GPT4有了更强大的处理复杂问题的能力，在普通的对话中可能体现不出来GPT4与chatGPT之间的区别，但是在复杂问题上，比如数学问题，或者需要多步推理的问题，GPT4的能力是显著优于chatGPT的。


openAI还用GPT4参加了人类的考试，在美国的高考、律师从业资格证都能取得前10%的成绩，而chatGPT参加这些考试的成绩在后10%-30%。


GPT4的多语言能力也得到了提升，除了英语，在一些小语种中都是优于chatGPT的。这里说一下我个人的之前的一个错误观点，因为在GPT3的训练数据中中文的占比仅为0.1%，我猜测chatGPT中中文的训练数据也不会太多，所以我一直以为在处理非英语问答时，chatGPT内部是不是翻译成了英文再去处理的，后来听了微软亚洲研究院周明博士的一个观点我觉得可能是正确的，他说世界的语言之间其实都是可以对齐的，模型在接受多语言训练数据时，他内部能够对齐不同语言之间的关系，也就是他即使不用翻译也能理解各种语言是什么意思，这可能就是GPT4为什么能很好处理多语言的原因吧。


最后就是openAI一直强调的，GPT4更加安全可控了，GPT4在去年8月份就训练好了，在今年3月份才发布，用一个比喻来说就是openAI这半年一直在驯服这头能力强大的野兽。他们用了基于人类反馈的强化学习让模型输出更符合人类要求的内容，尽量避免有毒的、性别歧视、种族歧视、黄暴等内容的生成。


## 总结 openAI不忘初心


我们总结一下吧，通过下面的表格，我们可以更清晰的看到这些模型之间的区别和他们这几年的发展路线。我们可以看到openAI一直沿着他们最初的想法在做，不断把模型做深做大，用到的数据越来越多，训练资源越来越大，当然他们的效果也越来越好，不断在打破我们的认知，希望我们国内也能早点做出能与之抗行的模型。


最后请把不忘初心打在公屏上。


![图17 总结](https://prod-files-secure.s3.us-west-2.amazonaws.com/d7dbc101-82ce-4f96-ae1a-879bd6c9f3a6/2730b1da-43a5-453f-a367-b98acf3d9590/gpt.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466XPXKZ2TL%2F20250222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250222T152217Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQC5ESFp%2FGyT3WeVHjwqJ5%2FwSfScJ9XX12hEXHTGs3CoJgIhAMMBomQJsZllNhjW7F%2B5CoWqKf6ZzaRWBVcG9OHuQWqnKogECOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMNjM3NDIzMTgzODA1IgyrWFEJjd04JdZOpG8q3APFSnObnIsn5mkMV5KnRBo%2FEqLPGHnpjU66vPcRMJzicRIiZPa7soPEttEEd9vphO6opaFJJQEivB5GQASjepOLw2Ptr30CO5mDu3XXwY0I%2B2oCam3tPB6BFSIo8hgnzjFi5Qk0qXTqo%2FMaauwVaCwPk8GBGyZuYYtg9Tsd6wJLXlUMB1CcZlS7TkdJzClCDVNDKbJlGK9ZnngNLa29zz11wrogwPimoRnlAuuCM8e3xzLewTBfVZczkC8s0%2BlSe%2Bk5YzgDC01M3wAfSK9QffMNvTKSB21hKiavkmujD%2BYVimkuR3qSH0LiZKevs%2F7dUrRrBaYJwg2poXbV7iXWRKRZsQL%2F8xjAfonGJW7z0%2BudcDWHBNKm30hTDwO56sZ5qQAoOBdU%2BJCfYBsxn8QWYqTJ%2FWwCd%2F7zoKm1tXLX7vF%2B%2BthUaR2GEDrVsEpxIeqoUp1cPz3GZXP0uogcZk764bQ308VrJ9jcTJpPV8KLUK9z%2FTUGZSyXaz8KDsWN1aFL413AbkK93crhTUkCPc9EUUAJ6jB8qCkauRWRRIAZlND6WAsxfn99%2FKpth3fgaV88hN%2Fn7riNlB8n98lEwxmiceMIe7mmyuJMLiWJsAu1hEslIDjeJT3xyApByfK9CDDf6ea9BjqkAWA31TplRdRZoC4NvH03CGNN4ySeEDC9at3kOYFYj6Yr24Tlm%2FvnWhhfss9ATG5pkvTXqfBJnlaDlgzc8qfq7GYQxWb7t7TQkKsg76E3ryAx3IfF3XALoxLxBVOCSPRefug3BKV0JxnN2BOlpemo7HL7gXkQS1TnFFLAnB8%2Biw2WUdIqnbxWSNW4jvEKldVnmfyTYrKlAQC62vZcVAO14gcbPXv4&X-Amz-Signature=794ccd7f4a6e364c0c8a70344573414c6ec005c1702e25e501c5bc8c82176b10&X-Amz-SignedHeaders=host&x-id=GetObject)


## 参考文献


[1] [Vaswani A , Shazeer N , Parmar N , et al. Attention Is All You Need[J]. 2017.](http://link.zhihu.com/?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1706.03762.pdf)


[2] [Devlin J , Chang M W , Lee K , et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[J]. 2018.](http://link.zhihu.com/?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1810.04805.pdf)


[3] [Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training[J]. 2018.](http://link.zhihu.com/?target=https%3A%2F%2Fwww.cs.ubc.ca%2F~amuham01%2FLING530%2Fpapers%2Fradford2018improving.pdf)


[4] [Radford A, Wu J, Child R, et al. Language models are unsupervised multitask learners[J]. OpenAI blog, 2019, 1(8): 9.](https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf)


[5] [Brown T, Mann B, Ryder N, et al. Language models are few-shot learners[J]. Advances in neural information processing systems, 2020, 33: 1877-1901.](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)


[6] [Kojima T, Gu S S, Reid M, et al. Large language models are zero-shot reasoners[J]. arXiv preprint arXiv:2205.11916, 2022.](https://arxiv.org/abs/2205.11916)


[7] [Wei J, Tay Y, Bommasani R, et al. Emergent abilities of large language models[J]. arXiv preprint arXiv:2206.07682, 2022.](https://arxiv.org/abs/2206.07682)


[8] [Wei J, Bosma M, Zhao V Y, et al. Finetuned language models are zero-shot learners[J]. arXiv preprint arXiv:2109.01652, 2021.](https://arxiv.org/abs/2109.01652)


[9] [openAI GPT4](https://openai.com/product/gpt-4)


[10] [Fu, Yao; Peng, Hao and Khot, Tushar. 拆解追溯 GPT-3.5 各项能力的起源. 2022](/360081d91ec245f29029d37b54573756)

